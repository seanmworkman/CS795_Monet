{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a5268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from matplotlib import pyplot as plt\n",
    "import pathlib\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d6901",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d476ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(256,256,3)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ba9a8",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d49557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 32x32 image\n",
    "    n_nodes = 128 * 64 * 64\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((64, 64, 128)))\n",
    "    # upsample to 128x128\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 256x256\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', activation='tanh'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(3, (8,8), activation='sigmoid', padding='same'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ab5f48",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc40eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa1c93f",
   "metadata": {},
   "source": [
    "# Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf0406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.io.decode_jpeg(image,channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, [256,256])\n",
    "    image = tf.reshape(image, [256,256,3])\n",
    "    return image\n",
    "\n",
    "# load and prepare monet training images\n",
    "def load_real_samples_monet():\n",
    "    monet_images = pathlib.Path('/home/sean/.keras/datasets/monet_jpg')\n",
    "    monet_images_list_ds = tf.data.Dataset.list_files(str(monet_images/'*'))\n",
    "    monet_ds = []\n",
    "    for i in monet_images_list_ds:\n",
    "        monet_ds.append(process_path(i))\n",
    "\n",
    "    trainX = tf.convert_to_tensor(monet_ds)\n",
    "    plot_samples(trainX, title='Monet Images\\n')\n",
    "    # print(trainX.shape)\n",
    "    # expand to 3d, e.g. add channels dimension\n",
    "    X = expand_dims(trainX, axis=-1)\n",
    "    # convert from unsigned ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0,255] to [0,1]\n",
    "    X = X / 255.0\n",
    "    return X\n",
    "\n",
    "# load and prepare photo training images\n",
    "def load_real_samples_photo():\n",
    "    photo_images = pathlib.Path('/home/sean/.keras/datasets/photo_jpg_cut')\n",
    "    photo_images_list_ds = tf.data.Dataset.list_files(str(photo_images/'*'))\n",
    "    photo_ds = []\n",
    "    for i in photo_images_list_ds:\n",
    "        photo_ds.append(process_path(i))\n",
    "\n",
    "    trainX = tf.convert_to_tensor(photo_ds)\n",
    "    plot_samples(trainX, title='Photo Images\\n')\n",
    "    # expand to 3d, e.g. add channels dimension\n",
    "    X = expand_dims(trainX, axis=-1)\n",
    "    # convert from unsigned ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0,255] to [0,1]\n",
    "    X = X / 255.0\n",
    "    return X\n",
    " \n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # print('fake samples:,',X.shape)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "# create and save a plot of generated images (reversed grayscale)\n",
    "def save_plot(examples, epoch, n=10):\n",
    "    # plot images\n",
    "    for i in range(n * n):\n",
    "        # define subplot\n",
    "        plt.subplot(n, n, 1 + i)\n",
    "        # turn off axis\n",
    "        plt.axis('off')\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
    "    # save plot to file\n",
    "    filename = 'generated_plot_e%03d.png' % (epoch+1)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def plot_samples(input_array, rows=4, cols=5, title=''):\n",
    "    '''\n",
    "    Function to plot 256x256 pixel drawings that are stored in a numpy array.\n",
    "    Specify how many rows and cols of pictures to display (default 4x5).  \n",
    "    If the array contains less images than subplots selected, surplus subplots remain empty.\n",
    "    '''    \n",
    "    fig, ax = plt.subplots(figsize=(cols,rows))\n",
    "    ax.axis('off')\n",
    "    plt.title(title)\n",
    "\n",
    "    for i in list(range(0, min(len(input_array),(rows*cols)) )):      \n",
    "        a = fig.add_subplot(rows,cols,i+1)\n",
    "        imgplot = plt.imshow(input_array[i,:784].reshape((256,256,3)), cmap='gray_r', interpolation='nearest')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        \n",
    "    plt.show()\n",
    " \n",
    "# evaluate the discriminator, plot generated images, save generator model\n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "    # prepare real samples\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    plot_samples(x_fake, title='Monet Images\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d54ca8c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d9f95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_modelA, d_modelA, gan_modelA, datasetA, g_modelB, d_modelB, gan_modelB, datasetB, latent_dim, n_epochs=100, n_batch=15):\n",
    "    bat_per_epo = int(datasetA.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # print(dataset.shape[0])\n",
    "    # print(n_batch)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            # get randomly selected 'real' samples from A\n",
    "            X_realA, y_realA = generate_real_samples(datasetA, half_batch)\n",
    "            # get randomly selected 'real' samples from B\n",
    "            X_realB, y_realB = generate_real_samples(datasetB, half_batch)\n",
    "            \n",
    "            # generate 'fake' examples for A\n",
    "            X_fakeA, y_fakeA = generate_fake_samples(g_modelA, latent_dim, half_batch)\n",
    "            # generate 'fake' examples for B\n",
    "            X_fakeB, y_fakeB = generate_fake_samples(g_modelB, latent_dim, half_batch)\n",
    "            \n",
    "            # create training set for the discriminator A\n",
    "            X_realA = X_realA.reshape(X_fakeA.shape)\n",
    "            X_fakeA = X_fakeA.reshape(X_fakeA.shape)\n",
    "            # create training set for the discriminator B\n",
    "            X_realB = X_realB.reshape(X_fakeB.shape)\n",
    "            X_fakeB = X_fakeB.reshape(X_fakeB.shape)\n",
    "            \n",
    "            XA, yA = vstack((X_realA, X_fakeA)), vstack((y_realA, y_fakeA))\n",
    "            XB, yB = vstack((X_realB, X_fakeB)), vstack((y_realB, y_fakeB))\n",
    "            \n",
    "            \n",
    "            # update discriminator model weights for A\n",
    "            d_lossA = d_modelA.train_on_batch(XA, yA)\n",
    "            # update discriminator model weights for B\n",
    "            d_lossB = d_modelB.train_on_batch(XB, yB)\n",
    "                                                        \n",
    "            # prepare points in latent space as input for the generator for A\n",
    "            X_ganA = generate_latent_points(latent_dim, n_batch)\n",
    "            # prepare points in latent space as input for the generator for B\n",
    "            X_ganB = generate_latent_points(latent_dim, n_batch)\n",
    "                                                        \n",
    "            # create inverted labels for the fake samples for A                                    \n",
    "            y_ganA = ones((n_batch, 1))\n",
    "            # create inverted labels for the fake samples for B                                 \n",
    "            y_ganB = ones((n_batch, 1))\n",
    "                                                        \n",
    "            # update the generator via the discriminator's error for A\n",
    "            g_lossA = gan_modelA.train_on_batch(X_ganA, y_ganA)\n",
    "            # update the generator via the discriminator's error for B\n",
    "            g_lossB = gan_modelB.train_on_batch(X_ganB, y_ganB)\n",
    "                                                        \n",
    "            # summarize loss on this batch for A\n",
    "            print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_lossA, g_lossA))\n",
    "            # summarize loss on this batch for B\n",
    "            print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_lossB, g_lossB))\n",
    "                                                        \n",
    "        # evaluate the model performance, sometimes\n",
    "        if (i+1) % 1 == 0:\n",
    "            summarize_performance(i, g_modelA, d_modelA, datasetA, latent_dim)\n",
    "            summarize_performance(i, g_modelB, d_modelB, datasetB, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910bccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONET DISCRIMINATOR\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 128, 128, 64)      1792      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 262144)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 262145    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 300,865\n",
      "Trainable params: 300,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONET GENERATOR\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 524288)            52953088  \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 524288)            0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 128, 128, 128)    262272    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 128, 128, 128)     0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 256, 256, 128)    262272    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 256, 256, 128)     0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 256, 256, 3)       24579     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,502,211\n",
      "Trainable params: 53,502,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "\n",
    "# create the monet discriminator\n",
    "d_model_monet = define_discriminator()\n",
    "print('MONET DISCRIMINATOR')\n",
    "d_model_monet.summary()\n",
    "# create the monet generator\n",
    "g_model_monet = define_generator(latent_dim)\n",
    "print('MONET GENERATOR')\n",
    "g_model_monet.summary()\n",
    "\n",
    "\n",
    "# create the photo discriminator\n",
    "d_model_photo = define_discriminator()\n",
    "print('PHOTO DISCRIMINATOR')\n",
    "d_model_photo.summary()\n",
    "# create the photo generator\n",
    "g_model_photo = define_generator(latent_dim)\n",
    "print('PHOTO GENERATOR')\n",
    "g_model_photo.summary()\n",
    "\n",
    "# create the monet gan\n",
    "gan_model_monet = define_gan(g_model_monet, d_model_photo)\n",
    "print('MONET GAN')\n",
    "gan_model_monet.summary()\n",
    "# create the photo gan\n",
    "gan_model_photo = define_gan(g_model_photo, d_model_monet)\n",
    "print('PHOTO GAN')\n",
    "gan_model_photo.summary()\n",
    "\n",
    "# load image data\n",
    "monet_dataset = load_real_samples_monet()\n",
    "photo_dataset = load_real_samples_photo()\n",
    "\n",
    "# train model\n",
    "train(g_model_monet, d_model_monet, gan_model_monet, monet_dataset, g_model_photo, d_model_photo, gan_model_photo, photo_dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6169d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
